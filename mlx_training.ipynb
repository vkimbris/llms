{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.mlx.models.gpt_mlx import GPT\n",
    "from src.mlx.models.gpt_mlx import generate_attention_mask\n",
    "from src.mlx.models.gpt_mlx import GPTParams\n",
    "from src.mlx.utils.mlx_lm_dataset import LanguageModelingDataset\n",
    "\n",
    "from src.tokenizers.tokenizer import Tokenizer\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_pretrained(\"tokenizers/tokenizer_ru_toxics/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = GPTParams(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    context_size=128,\n",
    "    input_dim=312,\n",
    "    query_dim=32,\n",
    "    value_dim=32,\n",
    "    feed_forward_hidden_dim=1024,\n",
    "    n_heads=4,\n",
    "    n_decoder_blocks=6,\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "N_EPOCHS = 1\n",
    "LEARNING_RATE = 2e-5\n",
    "MASK = generate_attention_mask(params.context_size - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.load(\"preprocessed_datasets/ru_toxics_tokens\")\n",
    "tokens = mx.array(tokens.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = LanguageModelingDataset(tokens).to_dataloader(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(model, inputs, labels):\n",
    "    logits = model(inputs, MASK)\n",
    "    logits = logits.reshape(-1, logits.shape[2])\n",
    "    \n",
    "    return mx.mean(nn.losses.cross_entropy(logits, labels.flatten()))\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "loss_and_grad_fn = nn.value_and_grad(model, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    for batch in dataloader:\n",
    "        inputs, labels = batch.values()\n",
    "        inputs, labels = mx.array(inputs), mx.array(labels)\n",
    "\n",
    "        loss, grads = loss_and_grad_fn(model, inputs, labels)\n",
    "\n",
    "        optimizer.update(model, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
